{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Prediction with a Long-Short Term Memory RNN Model \n",
    "    Guide for reading this program:\n",
    "        \"\"\" ... \"\"\" : Used to explain general ideas or procedures that\n",
    "                        encompass multiple blocks of code\n",
    "        # ...       : Used to explain the following block of code\n",
    "    Function Breakdown:\n",
    "        getData     : Import and clean up the data from a .csv file\n",
    "        createModel : Create, train, and test the LSTM RNN model\n",
    "        main        : Call createModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "# Get rid of warning messages about soon-to-be depreciated functions\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "The prepuse of the get data function is to import and clean up our data. The steps we used to do this are as followed:\n",
    "1. Remove all the random NaN that show up from importing\n",
    "2. Split the data into train and test\n",
    "3. Remove the labels from the data\n",
    "4. Organize the data into batches of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "\n",
    "    importedData = pd.read_csv(\"alldata.csv\")\n",
    "    # 1\n",
    "    cleanedData = importedData[np.isfinite(importedData[\"meanHrECG\"])]\n",
    "    cleanedData = cleanedData.drop(columns=['Unnamed: 0', 'subject'])\n",
    "\n",
    "    # 2\n",
    "    test_data = cleanedData.sample(frac=0.2)\n",
    "    test_labels = test_data[['stress', 'amuse']]\n",
    "\n",
    "    train_data = cleanedData.drop(test_data.index)\n",
    "    train_labels = train_data[['stress', 'amuse']]\n",
    "\n",
    "    # 3\n",
    "    test_data.drop(test_data[[\"stress\", \"amuse\"]], axis=1, inplace=True)\n",
    "    train_data.drop(train_data[[\"stress\", \"amuse\"]], axis=1, inplace=True)\n",
    "\n",
    "    # 4\n",
    "    ret = []\n",
    "    for d in [train_data.values, test_data.values, train_labels.values, test_labels.values]:\n",
    "        res = []\n",
    "        st = 0\n",
    "        end = 1000\n",
    "\n",
    "        while end <= d.size:\n",
    "            res.append(d[st:end])\n",
    "\n",
    "            st += 1000\n",
    "            end += 1000\n",
    "\n",
    "        ret.append(np.array(res))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the LSTM\n",
    "In the create Model function, we created a LSTM by feeding in the training model to train the LSTM to predict the emotions of a person. The detailed steps are as followed. \n",
    "1. Set up the LSTM\n",
    "        a. We first need to create place holders for the X (inputs) and Y (outputs)\n",
    "            i. the X placeholder has dimension 1000 X 72. 1000 is for the batch size, and 72 is for the amount of attubutes there are (only expects int64 dtypes as values )\n",
    "            ii. the Y placeholder has dimension 1 X 2. 1 being the batch size (just label), and 2 being the different number of class (only 0 and 1). (also only expects int64 dtypes)\n",
    "        b. Next, we create a feed dictionary for the inputs and labels to be able to feed the data into the LSTM with the labels. \n",
    "        c. Finally, we create the layers for the LSTM. We decide to create a 2 layer stack LSTM because it would be the simipliest and easiest to train. Now that we have set up the LSTM, it's time to create the model\n",
    "2. Creating RNN\n",
    "        a. It's that simiple. We use this line of code to create the RNN with the 2 layer LSTM stacks\n",
    "            i. outputs, new_state = tf.nn.dynamic_rnn(stacked_lstm, x, initial_state=_initial_state)\n",
    "3. Applying the SoftMax Funtion\n",
    "        a. the Soft Max function allows us to take the outputs from the model and normalizes them to create a probability vector depicting whether or not a data row is amused or stressed.\n",
    "        b. After we apply the soft max function, we ran a few tests to make sure we didn't mess anything up. And we're good to plug in the final numbers\n",
    "4. Create a gradient descent optimizer\n",
    "        a. We used this optimizer to slowly train the weights of the LSTM cells so that the end results would have the best accuracy. \n",
    "        b. \n",
    "5. Test it with the test set\n",
    "Everythin said above is also said within the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "def createModel():\n",
    "    \"\"\"Create the machine learning model and train and test it\n",
    "       (This might as well have been called main for its function)\"\"\"\n",
    "\n",
    "    # Get the data from our source and split it into train and test data\n",
    "    data = getData()\n",
    "    train_data = data[0]\n",
    "    test_data = data[1]\n",
    "    train_labels = data[2]\n",
    "    test_labels = data[3]\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \"\"\"\n",
    "    Set up the LSTM model\n",
    "    It is a best practice to create placeholders before variable assignments when using TensorFlow.\n",
    "    Here we'll create placeholders for inputs (\"Xs\") and outputs (\"Ys\").\n",
    "    Placeholder 'X': represents the \"space\" allocated input.\n",
    "    Each input (row of our csv file) has 74 attributes that act as the input.\n",
    "    The 'shape' argument defines the tensor size by its dimensions.\n",
    "    1st dimension = 1000. Indicates that the batch size, is 1000 items long.\n",
    "    2nd dimension = 74. Indicates the number of attributes in a single row.\n",
    "    Placeholder 'Y': represents the final output or the labels.\n",
    "    2 possible classes (1, 2)\n",
    "    The 'shape' argument defines the tensor size by its dimensions.\n",
    "    1st dimension = None. Indicates that the batch size, can be of any size.\n",
    "    2nd dimension = 2. Indicates the number of targets/outcomes\n",
    "    dtype for both placeholders: if you not sure, use tf.float32. The limitation here is that the later\n",
    "    presented softmax function only accepts float32 or float64 dtypes.\n",
    "    For more dtypes, check TensorFlow's documentation here\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Set up various variables for the project\"\"\"\n",
    "    max_grad_norm = 5\n",
    "\n",
    "    # The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "    num_steps = 74\n",
    "    # The number of processing units (neurons) in the hidden layers\n",
    "    hidden_size_l1 = 256\n",
    "    hidden_size_l2 = 128\n",
    "\n",
    "    # The size for each batch of data\n",
    "    batch_size = 1000\n",
    "    # The size of our vocabulary\n",
    "    vocab_size = 2\n",
    "\n",
    "    # Create the x and y variables using our data\n",
    "    x = train_data[0]\n",
    "    y = train_labels[0]\n",
    "    reshape_value = [1000,1,74]\n",
    "    x = x.reshape(reshape_value)\n",
    "    #y = y.reshape(reshape_value)\n",
    "\n",
    "    _input_data = tf.placeholder(tf.int64, [batch_size, 1, num_steps])\n",
    "    _targets = tf.placeholder(tf.int64, [batch_size, 2])\n",
    "\n",
    "    # Assemble x and y into a feed_dict\n",
    "    feed_dict = {_input_data: x, _targets: y}\n",
    "\n",
    "\n",
    "    #sess.run(_input_data, feed_dict)\n",
    "    \n",
    "    \n",
    "    # In this step, we create the stacked LSTM, which is a 2 layer LSTM network:\n",
    "    lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n",
    "    lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "    _initial_state = stacked_lstm.zero_state(batch_size, tf.float64)\n",
    "\n",
    "    #print(sess.run(_initial_state, feed_dict))\n",
    "\n",
    "    # Create the RNN\n",
    "    outputs, new_state = tf.nn.dynamic_rnn(stacked_lstm, x, initial_state=_initial_state)\n",
    "    print(outputs)\n",
    "\n",
    "    # Test run with a single data row just to make sure things are working\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(outputs[0], feed_dict)\n",
    "\n",
    "    \"\"\"Softmax is used to take the outputs from the model and normalizes them\n",
    "       to create a probability vector depicting whether or not a data row is \n",
    "       amused or stressed.\"\"\"\n",
    "\n",
    "    output = tf.reshape(outputs, [-1, hidden_size_l2])\n",
    "\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    logits = tf.matmul(tf.cast(output, tf.float32), softmax_w) + softmax_b\n",
    "\n",
    "    # Apply the softmax function\n",
    "    prob = tf.nn.softmax(logits)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_emotion_prob = sess.run(prob, feed_dict)\n",
    "    print(\"shape of the output: \", output_emotion_prob.shape)\n",
    "    print(\"The probability of observing words in t=0 to t=20\", output_emotion_prob[0:20])\n",
    "\n",
    "    # Run our model with actual test data to make sure things are going well\n",
    "    targ = sess.run(_targets, feed_dict)\n",
    "    print(targ[0])\n",
    "\n",
    "    print(tf.reshape(_targets, [-1]).shape)\n",
    "    print(logits.shape)\n",
    "    print(_targets)\n",
    "\n",
    "    # Calculate the loss of our data and model\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.slice(tf.reshape(_targets, [-1]), [0], [1000])],\n",
    "                                                              [tf.ones([batch_size])])\n",
    "\n",
    "    sess.run(loss, feed_dict)\n",
    "\n",
    "    cost = tf.reduce_sum(loss) / batch_size\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(cost, feed_dict))\n",
    "\n",
    "    lr = tf.Variable(.05, trainable=False)\n",
    "\n",
    "    \"\"\" The following lines create a gradient descent optimizer to \n",
    "        actually train our model to minimize the cost of our model\"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "\n",
    "    var_x = tf.placeholder(tf.float64)\n",
    "    var_y = tf.placeholder(tf.float64)\n",
    "    func_test = 2.0 * var_x * var_x + 3.0 * var_x * var_y\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(func_test, {var_x: 1.0, var_y: 2.0})\n",
    "\n",
    "    var_grad = tf.gradients(func_test, [var_x])\n",
    "    sess.run(var_grad, {var_x: 1.0, var_y: 2.0})\n",
    "\n",
    "    \"\"\"Find the gradient models that work best for this model\"\"\"\n",
    "    grad_t_list = tf.gradients(cost, tvars)\n",
    "\n",
    "    grad_t_list = [tf.cast(t, tf.float64) for t in grad_t_list]\n",
    "\n",
    "    grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "    sess.run(grads, feed_dict)\n",
    "\n",
    "    # Finangle the types so that all types are the same -> tf.float64\n",
    "    tvars = [tf.cast(t, tf.float64) for t in tvars]\n",
    "\n",
    "    # Run the optimizer to train our model\n",
    "    z = zip(grads, tvars)\n",
    "    train_op = optimizer.apply_gradients(z)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(train_op, feed_dict)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(_targets, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    \"\"\" Test our model using our test data \"\"\"\n",
    "    for i in range(20):\n",
    "        for step in range(len(train_data[1:49])):\n",
    "            x1 = train_data[step]\n",
    "            y1 = train_labels[step]\n",
    "            print(step)\n",
    "\n",
    "            reshape_value = [1000, 1, 74]\n",
    "            x1 = x1.reshape(reshape_value)\n",
    "            a = sess.run(train_op, feed_dict={_input_data: x1, _targets: y1})\n",
    "            print(sess.run(accuracy, feed_dict={_input_data: x1, _targets: y1}))\n",
    "\n",
    "\n",
    "    for step in range(len(test_data[:12])):\n",
    "        x1 = test_data[step]\n",
    "        y1 = test_labels[step]\n",
    "        print(step, len(test_data))\n",
    "\n",
    "        reshape_value = [1000, 1, 74]\n",
    "        x1 = x1.reshape(reshape_value)\n",
    "        print(sess.run(accuracy, feed_dict={_input_data: x1, _targets: y1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    createModel()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
